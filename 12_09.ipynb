{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXaFzkJHtY2Wac/yu0e/o2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashanth741/NLP-LAB/blob/main/12_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Example tweets dataset\n",
        "tweets = [\n",
        "    \"I love NLP! #AI\",\n",
        "    \"Deep learning is amazing. @user\",\n",
        "    \"Worst product ever, I hate it!!!\",\n",
        "    \"Good service and friendly staff :)\"\n",
        "]\n",
        "\n",
        "# Cleaning function\n",
        "def clean_tweet(text):\n",
        "    text = text.lower()                             # lowercase\n",
        "    text = re.sub(r'@\\w+', '', text)                # remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)                # remove hashtags\n",
        "    text = re.sub(r'http\\S+', '', text)             # remove URLs\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)            # remove numbers/punct\n",
        "    text = ' '.join([w for w in text.split() if w not in stop_words])  # remove stopwords\n",
        "    return text\n",
        "\n",
        "# Apply cleaning\n",
        "cleaned_tweets = [clean_tweet(t) for t in tweets]\n",
        "print(\"Cleaned Tweets:\", cleaned_tweets)\n",
        "\n",
        "# Tokenization\n",
        "max_words = 5000   # vocabulary size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"\")\n",
        "tokenizer.fit_on_texts(cleaned_tweets)\n",
        "\n",
        "# Convert to sequences\n",
        "sequences = tokenizer.texts_to_sequences(cleaned_tweets)\n",
        "print(\"Tokenized Sequences:\", sequences)\n",
        "\n",
        "# Padding\n",
        "max_len = 10   # maximum length for padding\n",
        "padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
        "print(\"Padded Sequences:\\n\", padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRA7dXC9P8ds",
        "outputId": "535a6622-091c-44e3-89c7-348361bfb187"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tweets: ['love nlp', 'deep learning amazing', 'worst product ever hate', 'good service friendly staff']\n",
            "Tokenized Sequences: [[2, 3], [4, 5, 6], [7, 8, 9, 10], [11, 12, 13, 14]]\n",
            "Padded Sequences:\n",
            " [[ 2  3  0  0  0  0  0  0  0  0]\n",
            " [ 4  5  6  0  0  0  0  0  0  0]\n",
            " [ 7  8  9 10  0  0  0  0  0  0]\n",
            " [11 12 13 14  0  0  0  0  0  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Use the cleaned tweets from Task 1\n",
        "cleaned_tweets = [\n",
        "    'love nlp',\n",
        "    'deep learning amazing',\n",
        "    'worst product ever hate',\n",
        "    'good service friendly staff'\n",
        "]\n",
        "\n",
        "# ----- CountVectorizer -----\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_count = count_vectorizer.fit_transform(cleaned_tweets)\n",
        "\n",
        "print(\"CountVectorizer Vocabulary:\\n\", count_vectorizer.vocabulary_)\n",
        "print(\"\\nCountVectorizer Feature Matrix:\\n\", X_count.toarray())\n",
        "\n",
        "# ----- TF-IDF Vectorizer -----\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_tweets)\n",
        "\n",
        "print(\"\\nTF-IDF Vocabulary:\\n\", tfidf_vectorizer.vocabulary_)\n",
        "print(\"\\nTF-IDF Feature Matrix:\\n\", X_tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQtuYOFBQSRt",
        "outputId": "7d3c296a-bb65-4cd6-8024-3ac80109b61a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer Vocabulary:\n",
            " {'love': 7, 'nlp': 8, 'deep': 1, 'learning': 6, 'amazing': 0, 'worst': 12, 'product': 9, 'ever': 2, 'hate': 5, 'good': 4, 'service': 10, 'friendly': 3, 'staff': 11}\n",
            "\n",
            "CountVectorizer Feature Matrix:\n",
            " [[0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
            " [1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 1 0 0 0 1 0 0 1]\n",
            " [0 0 0 1 1 0 0 0 0 0 1 1 0]]\n",
            "\n",
            "TF-IDF Vocabulary:\n",
            " {'love': 7, 'nlp': 8, 'deep': 1, 'learning': 6, 'amazing': 0, 'worst': 12, 'product': 9, 'ever': 2, 'hate': 5, 'good': 4, 'service': 10, 'friendly': 3, 'staff': 11}\n",
            "\n",
            "TF-IDF Feature Matrix:\n",
            " [[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.70710678 0.70710678 0.         0.         0.\n",
            "  0.        ]\n",
            " [0.57735027 0.57735027 0.         0.         0.         0.\n",
            "  0.57735027 0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.5        0.         0.         0.5\n",
            "  0.         0.         0.         0.5        0.         0.\n",
            "  0.5       ]\n",
            " [0.         0.         0.         0.5        0.5        0.\n",
            "  0.         0.         0.         0.         0.5        0.5\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Conv1D, LSTM, Dropout\n",
        "\n",
        "# Example labels for demo (replace with your dataset labels)\n",
        "y = np.array([1, 1, 0, 1])   # 1=positive, 0=negative\n",
        "\n",
        "# Use padded sequences from Task 1\n",
        "# (re-using \"padded\" from preprocessing step)\n",
        "X = padded\n",
        "\n",
        "# Train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Common parameters\n",
        "vocab_size = 5000   # same as tokenizer\n",
        "embedding_dim = 50\n",
        "max_len = X.shape[1]\n",
        "\n",
        "# ----- 1. MLP (Averaged Embeddings) -----\n",
        "mlp_model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "mlp_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "print(\"\\nTraining MLP...\")\n",
        "mlp_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=16)\n",
        "\n",
        "# ----- 2. CNN (1D) -----\n",
        "cnn_model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    Conv1D(128, 5, activation=\"relu\"),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "print(\"\\nTraining CNN...\")\n",
        "cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=16)\n",
        "\n",
        "# ----- 3. LSTM -----\n",
        "lstm_model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "lstm_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "print(\"\\nTraining LSTM...\")\n",
        "lstm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVdV2XTdQXyi",
        "outputId": "00e62c37-7d4a-40d3-957b-d8994173657c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MLP...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.3333 - loss: 0.6940 - val_accuracy: 1.0000 - val_loss: 0.6843\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.6667 - loss: 0.6889 - val_accuracy: 1.0000 - val_loss: 0.6781\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.6667 - loss: 0.6895 - val_accuracy: 1.0000 - val_loss: 0.6730\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.6667 - loss: 0.6900 - val_accuracy: 1.0000 - val_loss: 0.6684\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.6667 - loss: 0.6812 - val_accuracy: 1.0000 - val_loss: 0.6636\n",
            "\n",
            "Training CNN...\n",
            "Epoch 1/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.3333 - loss: 0.6929 - val_accuracy: 1.0000 - val_loss: 0.6757\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.6667 - loss: 0.6786 - val_accuracy: 1.0000 - val_loss: 0.6654\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.6667 - loss: 0.6782 - val_accuracy: 1.0000 - val_loss: 0.6560\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.6667 - loss: 0.6817 - val_accuracy: 1.0000 - val_loss: 0.6443\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.6667 - loss: 0.6680 - val_accuracy: 1.0000 - val_loss: 0.6313\n",
            "\n",
            "Training LSTM...\n",
            "Epoch 1/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.3333 - loss: 0.7031 - val_accuracy: 0.0000e+00 - val_loss: 0.6980\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.3333 - loss: 0.6990 - val_accuracy: 1.0000 - val_loss: 0.6874\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.3333 - loss: 0.6926 - val_accuracy: 1.0000 - val_loss: 0.6771\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.6667 - loss: 0.6911 - val_accuracy: 1.0000 - val_loss: 0.6665\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.6667 - loss: 0.6771 - val_accuracy: 1.0000 - val_loss: 0.6560\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7abf204ff6b0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "\n",
        "# --- Helper function ---\n",
        "def evaluate_model(model, X_test, y_test, deep=True):\n",
        "    if deep:  # for Keras models\n",
        "        y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    else:     # for scikit-learn models\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
        "    return acc, precision, recall, f1\n",
        "\n",
        "# --- Evaluate Deep Models ---\n",
        "mlp_metrics = evaluate_model(mlp_model, X_test, y_test, deep=True)\n",
        "cnn_metrics = evaluate_model(cnn_model, X_test, y_test, deep=True)\n",
        "lstm_metrics = evaluate_model(lstm_model, X_test, y_test, deep=True)\n",
        "\n",
        "# --- Classical ML with TF-IDF ---\n",
        "# Split TF-IDF features\n",
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train_tfidf, y_train_tfidf)\n",
        "log_reg_metrics = evaluate_model(log_reg, X_test_tfidf, y_test_tfidf, deep=False)\n",
        "\n",
        "# SVM\n",
        "svm = SVC(kernel=\"linear\")\n",
        "svm.fit(X_train_tfidf, y_train_tfidf)\n",
        "svm_metrics = evaluate_model(svm, X_test_tfidf, y_test_tfidf, deep=False)\n",
        "\n",
        "# --- Collect Results ---\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"MLP\", \"CNN\", \"LSTM\", \"Logistic Regression\", \"SVM\"],\n",
        "    \"Accuracy\": [mlp_metrics[0], cnn_metrics[0], lstm_metrics[0], log_reg_metrics[0], svm_metrics[0]],\n",
        "    \"Precision\": [mlp_metrics[1], cnn_metrics[1], lstm_metrics[1], log_reg_metrics[1], svm_metrics[1]],\n",
        "    \"Recall\": [mlp_metrics[2], cnn_metrics[2], lstm_metrics[2], log_reg_metrics[2], svm_metrics[2]],\n",
        "    \"F1-Score\": [mlp_metrics[3], cnn_metrics[3], lstm_metrics[3], log_reg_metrics[3], svm_metrics[3]]\n",
        "})\n",
        "\n",
        "print(\"\\n=== Evaluation Results ===\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T69vhMSiQilr",
        "outputId": "a89f7d3a-a7b7-4b48-9480-29c8112551c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\n",
            "=== Evaluation Results ===\n",
            "                 Model  Accuracy  Precision  Recall  F1-Score\n",
            "0                  MLP       1.0        1.0     1.0       1.0\n",
            "1                  CNN       1.0        1.0     1.0       1.0\n",
            "2                 LSTM       1.0        1.0     1.0       1.0\n",
            "3  Logistic Regression       1.0        1.0     1.0       1.0\n",
            "4                  SVM       1.0        1.0     1.0       1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumes \"results\" DataFrame is already created from Task 4\n",
        "print(\"\\n=== Final Results Table ===\")\n",
        "print(results)\n",
        "\n",
        "# Analysis function\n",
        "def analyze_results(results):\n",
        "    # Get best model by F1-score\n",
        "    best_model = results.loc[results[\"F1-Score\"].idxmax()]\n",
        "\n",
        "    print(\"\\n=== Brief Analysis ===\")\n",
        "\n",
        "    # 1. Embeddings vs TF-IDF\n",
        "    avg_deep_f1 = results.loc[results[\"Model\"].isin([\"MLP\",\"CNN\",\"LSTM\"]), \"F1-Score\"].mean()\n",
        "    avg_classical_f1 = results.loc[results[\"Model\"].isin([\"Logistic Regression\",\"SVM\"]), \"F1-Score\"].mean()\n",
        "    if avg_deep_f1 > avg_classical_f1:\n",
        "        print(f\"- Embeddings improved performance over TF-IDF \"\n",
        "              f\"({avg_deep_f1:.3f} vs {avg_classical_f1:.3f} F1-score).\")\n",
        "    else:\n",
        "        print(f\"- TF-IDF performed better or comparable to embeddings \"\n",
        "              f\"({avg_classical_f1:.3f} vs {avg_deep_f1:.3f} F1-score).\")\n",
        "\n",
        "    # 2. Best neural network\n",
        "    best_nn = results.loc[results[\"Model\"].isin([\"MLP\",\"CNN\",\"LSTM\"])].sort_values(\"F1-Score\", ascending=False).iloc[0]\n",
        "    print(f\"- Among neural networks, {best_nn['Model']} benefited most from embeddings \"\n",
        "          f\"(F1={best_nn['F1-Score']:.3f}).\")\n",
        "\n",
        "    # 3. Sequential models (LSTM) vs CNN/MLP\n",
        "    lstm_f1 = results.loc[results[\"Model\"]==\"LSTM\",\"F1-Score\"].values[0]\n",
        "    cnn_f1 = results.loc[results[\"Model\"]==\"CNN\",\"F1-Score\"].values[0]\n",
        "    mlp_f1 = results.loc[results[\"Model\"]==\"MLP\",\"F1-Score\"].values[0]\n",
        "\n",
        "    if lstm_f1 > max(cnn_f1, mlp_f1):\n",
        "        print(f\"- LSTM (F1={lstm_f1:.3f}) outperformed CNN (F1={cnn_f1:.3f}) and MLP (F1={mlp_f1:.3f}), \"\n",
        "              \"suggesting sequential models capture tweet context better.\")\n",
        "    else:\n",
        "        print(f\"- LSTM (F1={lstm_f1:.3f}) did not clearly outperform CNN (F1={cnn_f1:.3f}) or MLP (F1={mlp_f1:.3f}).\")\n",
        "\n",
        "    print(f\"\\n=> Best overall model: {best_model['Model']} with \"\n",
        "          f\"F1={best_model['F1-Score']:.3f}, Accuracy={best_model['Accuracy']:.3f}\")\n",
        "\n",
        "# Run analysis\n",
        "analyze_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5oXpUvFQolz",
        "outputId": "d1a754de-0413-49a5-8194-10257c93a12b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Final Results Table ===\n",
            "                 Model  Accuracy  Precision  Recall  F1-Score\n",
            "0                  MLP       1.0        1.0     1.0       1.0\n",
            "1                  CNN       1.0        1.0     1.0       1.0\n",
            "2                 LSTM       1.0        1.0     1.0       1.0\n",
            "3  Logistic Regression       1.0        1.0     1.0       1.0\n",
            "4                  SVM       1.0        1.0     1.0       1.0\n",
            "\n",
            "=== Brief Analysis ===\n",
            "- TF-IDF performed better or comparable to embeddings (1.000 vs 1.000 F1-score).\n",
            "- Among neural networks, MLP benefited most from embeddings (F1=1.000).\n",
            "- LSTM (F1=1.000) did not clearly outperform CNN (F1=1.000) or MLP (F1=1.000).\n",
            "\n",
            "=> Best overall model: MLP with F1=1.000, Accuracy=1.000\n"
          ]
        }
      ]
    }
  ]
}